Got it. I‚Äôll give you a **clean, professional, GitHub-ready README** that accurately reflects **what this project actually does**, how it‚Äôs built, and why it matters ‚Äî without over-claiming or hand-wavy buzzwords.

You can copy-paste this directly into `README.md` in your repo.

---

# ü©∫ Medevac-Gemma

**Offline, voice-activated Tactical Combat Casualty Care (TCCC) AI assistant**
Built for military combat medics, field experimentation, and applied medical AI research.

Medevac-Gemma is a fully local **speech-to-speech** system that listens to a medic‚Äôs spoken casualty report, transcribes it using a military-tuned ASR model, reasons over it with a fine-tuned MedGemma LLM, and responds with **structured, radio-concise TCCC guidance** ‚Äî all **without internet access**.

---

## ‚ú® Key Features

* üéô **Hands-free voice activation**

  * Wake word: **‚ÄúMedEvac-Gemma‚Äù**
  * End transmission with: **‚Äúover‚Äù**

* üß† **Medical reasoning via MedGemma**

  * GGUF-based, quantized LLM
  * Structured output (Assessment / Action / Warning)
  * Deterministic, low-temperature responses

* üó£ **Speech-to-speech loop**

  * Medic ‚Üí AI ‚Üí Medic ‚Üí AI (continuous conversation)
  * macOS TTS for spoken responses

* üì¥ **100% offline**

  * No cloud calls
  * No telemetry
  * No network dependency

* üçé **Optimized for Apple Silicon**

  * Tested on Mac mini M1
  * llama.cpp with Metal GPU offload

---

## üß© System Architecture

```
Microphone
   ‚Üì
Military ASR (medasr-mil / Faster-Whisper)
   ‚Üì
Conversation Manager
   ‚Üì
MedGemma LLM (llama.cpp / GGUF)
   ‚Üì
Structured TCCC Response
   ‚Üì
macOS Text-to-Speech
```

---

## üìÅ Project Structure

```text
Medevac-Gemma/
‚îú‚îÄ‚îÄ main.py        # Voice loop + conversation orchestration
‚îú‚îÄ‚îÄ audio.py       # Continuous microphone capture
‚îú‚îÄ‚îÄ stt.py         # Speech-to-text (medasr-mil)
‚îú‚îÄ‚îÄ llm.py         # MedGemma + llama.cpp interface
‚îî‚îÄ‚îÄ models/
    ‚îî‚îÄ‚îÄ medgemma/
        ‚îî‚îÄ‚îÄ medgemma-1.5-4b-it-Q4_K_M.gguf
```

---

## üõ† Requirements

### Hardware

* Apple Silicon Mac (M1/M2/M3 recommended)
* Microphone (built-in is fine)

### Software

* Python 3.9+
* `llama.cpp` built with Metal support
* macOS (for `say` TTS)

---

## üì¶ Python Dependencies

Install once:

```bash
pip install sounddevice numpy faster-whisper
```

macOS may prompt for microphone permissions ‚Äî allow access.

---

## ü¶ô llama.cpp Setup

You **do not** need to reinstall llama.cpp if it already works.

This project expects the `llama-simple-chat` binary, typically at:

```text
~/fiercecoyote/llama.cpp/build/bin/llama-simple-chat
```

Confirm it works:

```bash
/path/to/llama-simple-chat \
  -m /path/to/medgemma-1.5-4b-it-Q4_K_M.gguf \
  -c 2048 \
  -ngl 35
```

Update `llm.py` with **absolute paths** to:

* `LLAMA_BIN`
* `MODEL_PATH`

---

## ‚ñ∂Ô∏è Running Medevac-Gemma

From the project root:

```bash
python main.py
```

### Voice Flow

1. Program starts listening silently
2. Medic says:

   > **‚ÄúMedEvac-Gemma‚Äù**
3. System responds:

   > *‚ÄúGo ahead‚Äù*
4. Medic speaks scenario
5. Medic says:

   > **‚Äúover‚Äù**
6. AI responds with structured TCCC guidance (spoken + printed)
7. Repeat as needed

---

## üìã Output Format

The AI responds **only** in the following structure:

```
ASSESSMENT:
<brief medical assessment>

ACTION:
<step-by-step immediate actions>

WARNING:
<critical risks or red flags>
```

Designed for:

* radio brevity
* cognitive load reduction
* field usability

---

## üì¥ Offline Operation

Medevac-Gemma runs fully offline once models are present locally:

* ‚úÖ Local ASR model
* ‚úÖ Local LLM (GGUF)
* ‚úÖ Local inference via llama.cpp

No Wi-Fi, cellular, or cloud services required.

---

## ‚ö†Ô∏è Disclaimer

This project is **experimental** and **not a medical device**.

It is intended for:

* research
* training
* prototyping
* human-in-the-loop decision support

All medical decisions remain the responsibility of the human operator.

---

## üöÄ Future Work

Planned or possible extensions:

* Push-to-talk fallback mode
* Voice Activity Detection (VAD)
* Streaming token-level TTS
* Persistent llama.cpp process
* Encrypted conversation logging
* Jetson / ARM deployment
* Body-worn or vehicle-mounted integration

---

## üë§ Author

**CharlieKingOfTheRats**
Applied AI ‚Ä¢ Medical AI ‚Ä¢ Defense-adjacent systems

---

If you want, next I can:

* tailor this README for **Kaggle**
* add **architecture diagrams**
* add **demo transcripts**
* or write a **short project rationale / motivation section**

Just tell me.
